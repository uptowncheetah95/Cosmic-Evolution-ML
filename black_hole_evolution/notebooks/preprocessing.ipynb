{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccefcce3-a87c-469f-a21a-f4c6524702c9",
   "metadata": {},
   "source": [
    "# Black Hole Evolution Dataset Preparation\n",
    "\n",
    "This notebook extracts and formats time-series data from TNG100 to enable a LSTM to predict supermassive black hole evolution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519a0ea-3d48-4820-be17-91204072d33b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1. Environment Setup\n",
    "---\n",
    "Import necessary libraries and configure global settings for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4167640-e80f-43bd-b0f6-5d33958ea54c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.24.3\n",
      "PyTorch version: 2.0.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "random.seed(42)  # Ensures reproducible random sampling later\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a39f694-185b-4d85-9fc8-94975e09e4cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Load and Filter TNG100 Subhalo Catalog  \n",
    "---\n",
    "Locate the TNG100 simulation directory and loading the subhalo catalog from snapshot 33. We then extract all subhalos hosting supermassive black holes (SMBHs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522ef060-9069-4ff2-ae45-f9f21bbb9489",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2.1 Load Dataset\n",
    "---\n",
    "This cell loads the preprocessed black hole evolution dataset from the data directory and confirms its structure. It also sets the simulation base path for future data access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b7e544-0192-4c7c-a0d5-754e7f9ae387",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with shape: (37500, 8)\n",
      "Columns: ['subhalo_id', 'snapshot', 'bh_mass', 'bh_acc', 'stellar_mass', 'sfr', 'halo_mass', 'vel_disp']\n"
     ]
    }
   ],
   "source": [
    "import illustris_python as il\n",
    "import pandas as pd\n",
    "\n",
    "# Set simulation base path\n",
    "basePath = \"/home/tnguser/sims.TNG/TNG100-1\"\n",
    "\n",
    "# Load precompiled black hole sample from CSV\n",
    "csv_path = \"/home/tnguser/cosmic-evolution-ml/black_hole_evolution/data/black_hole_evolution_tng100.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "print(\"Columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886c4a12-1d3f-49ed-aa35-fdd241975b8c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.2 Save Processed Data as NumPy Arrays\n",
    "---\n",
    "This section converts the cleaned long-format CSV dataset into NumPy arrays for efficient model training and stores them alongside the CSV in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1c7cfd8-bb30-4145-860c-0893b7135b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Cleaned and saved arrays to: ../data\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "CSV_PATH = DATA_DIR / \"black_hole_evolution_tng100.csv\"  # long-format dataset\n",
    "\n",
    "# Load\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Numeric columns and sanitization\n",
    "NUM_COLS = [\"bh_mass\",\"bh_acc\",\"stellar_mass\",\"sfr\",\"halo_mass\",\"vel_disp\"]\n",
    "df[NUM_COLS] = df[NUM_COLS].apply(pd.to_numeric, errors=\"coerce\")\n",
    "df[NUM_COLS] = df[NUM_COLS].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# NaN imputation: per-snapshot median, then global median, final zero-fill safety\n",
    "for col in NUM_COLS:\n",
    "    med_by_snap = df.groupby(\"snapshot\")[col].transform(\"median\")\n",
    "    df[col] = df[col].fillna(med_by_snap)\n",
    "    df[col] = df[col].fillna(df[col].median())\n",
    "df[NUM_COLS] = df[NUM_COLS].fillna(0.0)\n",
    "\n",
    "# Save arrays + normalization stats\n",
    "ids        = df[\"subhalo_id\"].to_numpy()\n",
    "snapshots  = df[\"snapshot\"].to_numpy().astype(int)\n",
    "features   = df[NUM_COLS].to_numpy(dtype=np.float32)\n",
    "\n",
    "feat_mean = features.mean(axis=0)\n",
    "feat_std  = features.std(axis=0) + 1e-8\n",
    "\n",
    "np.save(DATA_DIR / \"ids.npy\", ids)\n",
    "np.save(DATA_DIR / \"snapshots.npy\", snapshots)\n",
    "np.save(DATA_DIR / \"features.npy\", features)\n",
    "np.save(DATA_DIR / \"feat_mean.npy\", feat_mean)\n",
    "np.save(DATA_DIR / \"feat_std.npy\",  feat_std)\n",
    "\n",
    "print(f\"[OK] Cleaned and saved arrays to: {DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4e53e7-233c-4707-925c-e05de7d54c98",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Data Loading\n",
    "---\n",
    "This section prepares the processed dataset for model training by defining a PyTorch-compatible `Dataset` and `DataLoader`. The goal is to efficiently feed the model sequential input–output pairs representing black hole and galaxy properties across snapshots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468838b6-9e2e-448b-91cc-85bea0ba593c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.1 Dataset and DataLoader Setup\n",
    "---\n",
    "We load the processed `.npy` files generated in Section 2, organize them into sequences of `(initial_conditions, final_conditions)`, and configure a `DataLoader` to support batching, shuffling, and efficient GPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cc34b9a0-75e0-41a8-9a32-4398af7ff695",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Train samples: 28000 | Val samples: 7000\n",
      "Batch shapes: torch.Size([64, 6]) torch.Size([64, 6])\n"
     ]
    }
   ],
   "source": [
    "# 3.1 — Dataset and DataLoader Setup (normalized, NaN-safe)\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path(\"../data\")\n",
    "\n",
    "class BlackHoleEvolutionDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Samples are (initial_conditions at t, final_conditions at t+1).\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir: Path, sequence_length: int = 2, normalize: bool = True):\n",
    "        self.ids        = np.load(data_dir / \"ids.npy\")\n",
    "        self.snapshots  = np.load(data_dir / \"snapshots.npy\")\n",
    "        self.features   = np.load(data_dir / \"features.npy\")  # [N_rows, F]\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        # Replace lingering NaNs just in case\n",
    "        nan_mask = np.isnan(self.features)\n",
    "        if nan_mask.any():\n",
    "            col_means = np.nanmean(self.features, axis=0)\n",
    "            self.features[nan_mask] = np.take(col_means, np.where(nan_mask)[1])\n",
    "\n",
    "        # Normalize using saved stats\n",
    "        if normalize:\n",
    "            mean = np.load(data_dir / \"feat_mean.npy\")\n",
    "            std  = np.load(data_dir / \"feat_std.npy\")\n",
    "            self.features = (self.features - mean) / std\n",
    "            self.features = np.nan_to_num(self.features, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "        # Group by subhalo and sort by snapshot\n",
    "        self.subhalo_sequences = {}\n",
    "        for sid in np.unique(self.ids):\n",
    "            m = self.ids == sid\n",
    "            seq_feat = self.features[m]\n",
    "            seq_snap = self.snapshots[m]\n",
    "            order = np.argsort(seq_snap)\n",
    "            self.subhalo_sequences[sid] = seq_feat[order]\n",
    "\n",
    "        # Build (t -> t+1) pairs\n",
    "        self.samples = []\n",
    "        L = self.sequence_length\n",
    "        for seq in self.subhalo_sequences.values():\n",
    "            if len(seq) >= L:\n",
    "                for i in range(len(seq) - L + 1):\n",
    "                    x0 = seq[i]          # t\n",
    "                    x1 = seq[i + L - 1]  # t+1 for L=2\n",
    "                    self.samples.append((x0, x1))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x, y = self.samples[idx]\n",
    "        x = torch.from_numpy(np.asarray(x, dtype=np.float32))\n",
    "        y = torch.from_numpy(np.asarray(y, dtype=np.float32))\n",
    "        x = torch.nan_to_num(x, nan=0.0)\n",
    "        y = torch.nan_to_num(y, nan=0.0)\n",
    "        return x, y\n",
    "\n",
    "# Dataset, split, loaders\n",
    "dataset = BlackHoleEvolutionDataset(DATA_DIR, sequence_length=2, normalize=True)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size   = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False)\n",
    "\n",
    "print(f\"[OK] Train samples: {len(train_ds)} | Val samples: {len(val_ds)}\")\n",
    "xb, yb = next(iter(train_loader))\n",
    "print(\"Batch shapes:\", xb.shape, yb.shape)  # [B, F], [B, F]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c8ba67-1628-4c1a-8a07-bc887ef47370",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.2 Model Architecture Definition\n",
    "---\n",
    "We define a neural network model to learn the mapping from `initial_conditions` to `final_conditions`. The architecture consists of fully connected layers with nonlinear activations, allowing the model to capture complex relationships in the astrophysical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a6c2fbf-ce9a-4716-b4a7-ebe2572b1055",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlackHoleEvolutionModel(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.1, inplace=False)\n",
      "    (3): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.1, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 3.2 — Model Architecture Definition (multi-output full state)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "class BlackHoleEvolutionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=None, p_drop=0.1):\n",
    "        super().__init__()\n",
    "        if output_dim is None:\n",
    "            output_dim = input_dim  # predict full vector\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Infer feature size F from a batch\n",
    "F = xb.shape[1]\n",
    "model = BlackHoleEvolutionModel(input_dim=F, hidden_dim=128, output_dim=F, p_drop=0.1)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f632284-c5cb-4f37-ade1-1369ee014b24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.3 Loss Function & Optimizer Setup\n",
    "---\n",
    "We configure the loss function to measure prediction accuracy and the optimizer to update model weights. Mean Squared Error (MSE) is used since we are predicting continuous astrophysical quantities, and Adam is chosen for its adaptive learning rate capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ef9d71a-a6c8-4eca-b50d-9849ca37559f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss/optimizer ready.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "print(\"Loss/optimizer ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda56fc9-e1fb-4a4b-bb3f-4b90aa709ec0",
   "metadata": {},
   "source": [
    "#### 3.4 Training Loop\n",
    "---\n",
    "We iterate over the dataset for multiple epochs, performing forward passes, computing the loss, backpropagating gradients, and updating model parameters. Progress is printed each epoch to monitor convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f53b1fe7-2684-42c3-8258-c2edbdb407e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train 0.406638 | val 0.347688\n",
      "Epoch 02 | train 0.244567 | val 0.322296\n",
      "Epoch 03 | train 0.225229 | val 0.321199\n",
      "Epoch 04 | train 0.228716 | val 0.323741\n",
      "Epoch 05 | train 0.228901 | val 0.319949\n",
      "Epoch 06 | train 0.221880 | val 0.317177\n",
      "Epoch 07 | train 0.218889 | val 0.316769\n",
      "Epoch 08 | train 0.215312 | val 0.321200\n",
      "Epoch 09 | train 0.218081 | val 0.321519\n",
      "Epoch 10 | train 0.206916 | val 0.318290\n",
      "Epoch 11 | train 0.207824 | val 0.317075\n",
      "Epoch 12 | train 0.213314 | val 0.318018\n",
      "Epoch 13 | train 0.216193 | val 0.317836\n",
      "Epoch 14 | train 0.213329 | val 0.317468\n",
      "Epoch 15 | train 0.206873 | val 0.317710\n",
      "Epoch 16 | train 0.210540 | val 0.321647\n",
      "Epoch 17 | train 0.215063 | val 0.321442\n",
      "Epoch 18 | train 0.211196 | val 0.318691\n",
      "Epoch 19 | train 0.208297 | val 0.316663\n",
      "Epoch 20 | train 0.204736 | val 0.317517\n"
     ]
    }
   ],
   "source": [
    "from math import isfinite\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "num_epochs = 20\n",
    "max_grad_norm = 1.0\n",
    "\n",
    "for epoch in range(1, num_epochs + 1):\n",
    "    # Train\n",
    "    model.train()\n",
    "    train_loss_sum = 0.0\n",
    "    n_train = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb = torch.nan_to_num(xb, nan=0.0).to(device)\n",
    "        yb = torch.nan_to_num(yb, nan=0.0).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        if not isfinite(loss.item()):\n",
    "            continue\n",
    "\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_sum += loss.item() * xb.size(0)\n",
    "        n_train += xb.size(0)\n",
    "\n",
    "    train_loss = train_loss_sum / max(n_train, 1)\n",
    "\n",
    "    # Validate\n",
    "    model.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    n_val = 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb = torch.nan_to_num(xb, nan=0.0).to(device)\n",
    "            yb = torch.nan_to_num(yb, nan=0.0).to(device)\n",
    "            preds = model(xb)\n",
    "            vloss = criterion(preds, yb)\n",
    "            if isfinite(vloss.item()):\n",
    "                val_loss_sum += vloss.item() * xb.size(0)\n",
    "                n_val += xb.size(0)\n",
    "\n",
    "    val_loss = val_loss_sum / max(n_val, 1)\n",
    "    print(f\"Epoch {epoch:02d} | train {train_loss:.6f} | val {val_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300a3169-13e9-45ac-aae0-22514492ad9f",
   "metadata": {},
   "source": [
    "#### 3.5 Validation Metrics per Feature\n",
    "---\n",
    "de-normalized to original units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15b68ca3-92a3-40a0-aee5-bd6a041f287f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-feature RMSE / MAE (original units):\n",
      "- bh_mass: RMSE=3.4844e-04 | MAE=6.9464e-05\n",
      "- bh_acc: RMSE=6.6590e-03 | MAE=8.2373e-04\n",
      "- stellar_mass: RMSE=1.9152e-01 | MAE=1.9138e-02\n",
      "- sfr: RMSE=5.3695e+00 | MAE=9.7986e-01\n",
      "- halo_mass: RMSE=1.9818e+01 | MAE=1.5357e+00\n",
      "- vel_disp: RMSE=6.8698e+00 | MAE=3.9400e+00\n"
     ]
    }
   ],
   "source": [
    "# 3.5 — Validation Metrics per Feature (de-normalized to original units)\n",
    "\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import torch\n",
    "\n",
    "FEATURES = [\"bh_mass\",\"bh_acc\",\"stellar_mass\",\"sfr\",\"halo_mass\",\"vel_disp\"]\n",
    "\n",
    "DATA_DIR = pathlib.Path(\"../data\")\n",
    "mean = np.load(DATA_DIR / \"feat_mean.npy\")\n",
    "std  = np.load(DATA_DIR / \"feat_std.npy\")\n",
    "\n",
    "model.eval()\n",
    "all_pred, all_true = [], []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in val_loader:\n",
    "        xb = torch.nan_to_num(xb, nan=0.0).to(device)\n",
    "        yb = torch.nan_to_num(yb, nan=0.0).to(device)\n",
    "        pred = model(xb)\n",
    "        all_pred.append(pred.cpu().numpy())\n",
    "        all_true.append(yb.cpu().numpy())\n",
    "\n",
    "P = np.concatenate(all_pred, axis=0)\n",
    "T = np.concatenate(all_true, axis=0)\n",
    "\n",
    "# Back to physical units\n",
    "P_real = P * std + mean\n",
    "T_real = T * std + mean\n",
    "\n",
    "rmse = np.sqrt(np.mean((P_real - T_real) ** 2, axis=0))\n",
    "mae  = np.mean(np.abs(P_real - T_real), axis=0)\n",
    "\n",
    "print(\"Per-feature RMSE / MAE (original units):\")\n",
    "for i, name in enumerate(FEATURES):\n",
    "    print(f\"- {name}: RMSE={rmse[i]:.4e} | MAE={mae[i]:.4e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccc7567-70b8-4a49-9745-17e4424d82e0",
   "metadata": {},
   "source": [
    "#### 3.6 Chepoint\n",
    "---\n",
    "Check to make sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "586da5a8-80d9-4681-8be9-bbf11abd19d6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Saved checkpoint → ../data/models/blackhole_evolution_model.pt\n",
      "[OK] Saved 5-step forecast → ../data/forecasts/forecast_subhalo_13.csv\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---- Paths (clean layout) ----\n",
    "DATA_ROOT   = Path(\"../data\")\n",
    "RAW_DIR     = DATA_ROOT / \"raw\"        # (not used here, kept for consistency)\n",
    "PROC_DIR    = DATA_ROOT / \"processed\"  # features/ids/snapshots .npy live here\n",
    "STATS_DIR   = DATA_ROOT / \"stats\"      # feat_mean.npy, feat_std.npy\n",
    "MODEL_DIR   = DATA_ROOT / \"models\"     # checkpoints\n",
    "FORECAST_DIR= DATA_ROOT / \"forecasts\"  # rollout outputs\n",
    "\n",
    "for d in [PROC_DIR, STATS_DIR, MODEL_DIR, FORECAST_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Config ----\n",
    "FEATURES   = [\"bh_mass\",\"bh_acc\",\"stellar_mass\",\"sfr\",\"halo_mass\",\"vel_disp\"]\n",
    "CKPT_PATH  = MODEL_DIR / \"blackhole_evolution_model.pt\"\n",
    "ROLL_K     = 5\n",
    "CLAMP_NONNEG = {\"bh_mass\",\"bh_acc\",\"stellar_mass\",\"sfr\",\"halo_mass\",\"vel_disp\"}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# ---- Save checkpoint ----\n",
    "ckpt = {\n",
    "    \"ts\": time.strftime(\"%Y-%m-%d_%H-%M-%S\"),\n",
    "    \"state_dict\": model.state_dict(),\n",
    "    \"feature_names\": FEATURES,\n",
    "    \"mean\": np.load(STATS_DIR / \"feat_mean.npy\"),\n",
    "    \"std\":  np.load(STATS_DIR / \"feat_std.npy\"),\n",
    "    \"input_dim\": next(iter(train_loader))[0].shape[1],  # F\n",
    "}\n",
    "torch.save(ckpt, CKPT_PATH)\n",
    "print(f\"[OK] Saved checkpoint → {CKPT_PATH}\")\n",
    "\n",
    "# ---- Rollout helpers ----\n",
    "_mean = ckpt[\"mean\"]\n",
    "_std  = ckpt[\"std\"]\n",
    "\n",
    "def _to_norm(x_real: np.ndarray) -> np.ndarray:\n",
    "    return (x_real - _mean) / _std\n",
    "\n",
    "def _to_real(x_norm: np.ndarray) -> np.ndarray:\n",
    "    return x_norm * _std + _mean\n",
    "\n",
    "@torch.no_grad()\n",
    "def rollout(model: nn.Module, x0_real: np.ndarray, k: int) -> np.ndarray:\n",
    "    \"\"\"Autoregressive k-step rollout; returns [k, F] in original units.\"\"\"\n",
    "    x = torch.tensor(_to_norm(x0_real), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    out = []\n",
    "    for _ in range(k):\n",
    "        y_norm = model(x)                        # [1, F] normalized\n",
    "        y_real = _to_real(y_norm.cpu().numpy().squeeze(0))\n",
    "        for name in CLAMP_NONNEG:                # simple physical clamp\n",
    "            j = FEATURES.index(name)\n",
    "            if y_real[j] < 0: y_real[j] = 0.0\n",
    "        out.append(y_real.copy())\n",
    "        x = torch.tensor(_to_norm(y_real), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    return np.vstack(out)\n",
    "\n",
    "# ---- Forecast example (single subhalo) ----\n",
    "def forecast_example(processed_csv: Path, k: int = ROLL_K) -> Path:\n",
    "    \"\"\"\n",
    "    Use last available snapshot of first subhalo in processed CSV to produce a k-step forecast.\n",
    "    Saves CSV to FORECAST_DIR and returns its path.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(processed_csv)\n",
    "    sid = int(df[\"subhalo_id\"].iloc[0])\n",
    "    sub = df[df[\"subhalo_id\"] == sid].sort_values(\"snapshot\")\n",
    "    last = sub.iloc[-1]\n",
    "    x0   = last[FEATURES].to_numpy(dtype=np.float32)\n",
    "\n",
    "    preds = rollout(model, x0, k)  # [k, F]\n",
    "    snaps = [int(last[\"snapshot\"]) + i for i in range(1, k+1)]\n",
    "    out   = pd.DataFrame(preds, columns=FEATURES)\n",
    "    out.insert(0, \"snapshot\", snaps)\n",
    "    out.insert(0, \"subhalo_id\", sid)\n",
    "\n",
    "    out_path = FORECAST_DIR / f\"forecast_subhalo_{sid}.csv\"\n",
    "    out.to_csv(out_path, index=False)\n",
    "    print(f\"[OK] Saved {k}-step forecast → {out_path}\")\n",
    "    return out_path\n",
    "\n",
    "# Run example (expects your long-format CSV to be in processed/)\n",
    "processed_csv_path = PROC_DIR / \"black_hole_evolution_tng100.csv\"\n",
    "_ = forecast_example(processed_csv_path, k=ROLL_K)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
