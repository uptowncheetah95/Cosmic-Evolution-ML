{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccefcce3-a87c-469f-a21a-f4c6524702c9",
   "metadata": {},
   "source": [
    "# Black Hole Evolution Dataset Preparation\n",
    "\n",
    "This notebook extracts and formats time-series data from TNG100 to enable a LSTM to predict supermassive black hole evolution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a519a0ea-3d48-4820-be17-91204072d33b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1. Environment Setup\n",
    "---\n",
    "Import necessary libraries and configure global settings for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4167640-e80f-43bd-b0f6-5d33958ea54c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.24.3\n",
      "PyTorch version: 2.0.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "random.seed(42)  # Ensures reproducible random sampling later\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a39f694-185b-4d85-9fc8-94975e09e4cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Load and Filter TNG100 Subhalo Catalog  \n",
    "---\n",
    "Locate the TNG100 simulation directory and loading the subhalo catalog from snapshot 33. We then extract all subhalos hosting supermassive black holes (SMBHs).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522ef060-9069-4ff2-ae45-f9f21bbb9489",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.1 Load Dataset\n",
    "---\n",
    "This cell loads the preprocessed black hole evolution dataset from the data directory and confirms its structure. It also sets the simulation base path for future data access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01b7e544-0192-4c7c-a0d5-754e7f9ae387",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with shape: (37500, 8)\n",
      "Columns: ['subhalo_id', 'snapshot', 'bh_mass', 'bh_acc', 'stellar_mass', 'sfr', 'halo_mass', 'vel_disp']\n"
     ]
    }
   ],
   "source": [
    "import illustris_python as il\n",
    "import pandas as pd\n",
    "\n",
    "# Set simulation base path\n",
    "basePath = \"/home/tnguser/sims.TNG/TNG100-1\"\n",
    "\n",
    "# Load precompiled black hole sample from CSV\n",
    "csv_path = \"/home/tnguser/cosmic-evolution-ml/black_hole_evolution/data/black_hole_evolution_tng100.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "print(f\"Dataset loaded with shape: {df.shape}\")\n",
    "print(\"Columns:\", df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886c4a12-1d3f-49ed-aa35-fdd241975b8c",
   "metadata": {},
   "source": [
    "#### 2.2 Save Processed Data as NumPy Arrays\n",
    "---\n",
    "This section converts the cleaned long-format CSV dataset into NumPy arrays for efficient model training and stores them alongside the CSV in the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1c7cfd8-bb30-4145-860c-0893b7135b0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Processed arrays saved to: ../data\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "CSV_PATH = DATA_DIR / \"black_hole_evolution_tng100.csv\"  # long-format dataset\n",
    "\n",
    "# Load long-format CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "ids = df[\"subhalo_id\"].to_numpy()\n",
    "snapshots = df[\"snapshot\"].to_numpy()\n",
    "features = df.drop(columns=[\"subhalo_id\", \"snapshot\"]).to_numpy()\n",
    "\n",
    "# Save arrays in the same directory as the CSV\n",
    "np.save(DATA_DIR / \"ids.npy\", ids)\n",
    "np.save(DATA_DIR / \"snapshots.npy\", snapshots)\n",
    "np.save(DATA_DIR / \"features.npy\", features)\n",
    "\n",
    "print(f\"[OK] Processed arrays saved to: {DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4e53e7-233c-4707-925c-e05de7d54c98",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Data Loading\n",
    "---\n",
    "This section prepares the processed dataset for model training by defining a PyTorch-compatible `Dataset` and `DataLoader`. The goal is to efficiently feed the model sequential inputâ€“output pairs representing black hole and galaxy properties across snapshots."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468838b6-9e2e-448b-91cc-85bea0ba593c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.1 Dataset and DataLoader Setup\n",
    "---\n",
    "We load the processed `.npy` files generated in Section 2, organize them into sequences of `(initial_conditions, final_conditions)`, and configure a `DataLoader` to support batching, shuffling, and efficient GPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc34b9a0-75e0-41a8-9a32-4398af7ff695",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 35000\n",
      "Batch X shape: torch.Size([64, 6])\n",
      "Batch Y shape: torch.Size([64, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class BlackHoleEvolutionDataset(Dataset):\n",
    "    def __init__(self, data_dir, sequence_length=2):\n",
    "        self.ids = np.load(data_dir / \"ids.npy\")\n",
    "        self.snapshots = np.load(data_dir / \"snapshots.npy\")\n",
    "        self.features = np.load(data_dir / \"features.npy\")\n",
    "        self.sequence_length = sequence_length\n",
    "        \n",
    "        # Replace NaNs with column means\n",
    "        nan_mask = np.isnan(self.features)\n",
    "        if nan_mask.any():\n",
    "            col_means = np.nanmean(self.features, axis=0)\n",
    "            self.features[nan_mask] = np.take(col_means, np.where(nan_mask)[1])\n",
    "        \n",
    "        # Group by subhalo_id\n",
    "        self.subhalo_sequences = {}\n",
    "        for sid in np.unique(self.ids):\n",
    "            mask = self.ids == sid\n",
    "            seq_features = self.features[mask]\n",
    "            seq_snapshots = self.snapshots[mask]\n",
    "            sort_idx = np.argsort(seq_snapshots)\n",
    "            self.subhalo_sequences[sid] = seq_features[sort_idx]\n",
    "\n",
    "        # Build input-output pairs\n",
    "        self.samples = []\n",
    "        for seq in self.subhalo_sequences.values():\n",
    "            if len(seq) >= self.sequence_length:\n",
    "                for i in range(len(seq) - self.sequence_length + 1):\n",
    "                    initial = seq[i]\n",
    "                    final = seq[i + self.sequence_length - 1]\n",
    "                    self.samples.append((initial, final))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        initial, final = self.samples[idx]\n",
    "        return torch.tensor(initial, dtype=torch.float32), torch.tensor(final, dtype=torch.float32)\n",
    "\n",
    "# Initialize dataset and dataloader\n",
    "dataset = BlackHoleEvolutionDataset(DATA_DIR, sequence_length=2)\n",
    "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "print(f\"Total samples: {len(dataset)}\")\n",
    "for batch in dataloader:\n",
    "    x, y = batch\n",
    "    print(\"Batch X shape:\", x.shape)\n",
    "    print(\"Batch Y shape:\", y.shape)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c8ba67-1628-4c1a-8a07-bc887ef47370",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.2 Model Architecture Definition\n",
    "---\n",
    "We define a neural network model to learn the mapping from `initial_conditions` to `final_conditions`. The architecture consists of fully connected layers with nonlinear activations, allowing the model to capture complex relationships in the astrophysical data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7a6c2fbf-ce9a-4716-b4a7-ebe2572b1055",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BlackHoleEvolutionModel(\n",
      "  (net): Sequential(\n",
      "    (0): Linear(in_features=6, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=6, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BlackHoleEvolutionModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, output_dim=None):\n",
    "        super().__init__()\n",
    "        if output_dim is None:\n",
    "            output_dim = input_dim  # Predict same number of features as input\n",
    "        \n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# Initialize model\n",
    "input_dim = dataset[0][0].shape[0]  # Feature size from initial_conditions\n",
    "model = BlackHoleEvolutionModel(input_dim=input_dim, hidden_dim=128)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f632284-c5cb-4f37-ade1-1369ee014b24",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 3.3 Loss Function & Optimizer Setup\n",
    "---\n",
    "We configure the loss function to measure prediction accuracy and the optimizer to update model weights. Mean Squared Error (MSE) is used since we are predicting continuous astrophysical quantities, and Adam is chosen for its adaptive learning rate capabilities.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ef9d71a-a6c8-4eca-b50d-9849ca37559f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss function and optimizer ready.\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "print(\"Loss function and optimizer ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda56fc9-e1fb-4a4b-bb3f-4b90aa709ec0",
   "metadata": {},
   "source": [
    "#### 3.4 Training Loop\n",
    "---\n",
    "We iterate over the dataset for multiple epochs, performing forward passes, computing the loss, backpropagating gradients, and updating model parameters. Progress is printed each epoch to monitor convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f53b1fe7-2684-42c3-8258-c2edbdb407e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: nan\n",
      "Epoch [2/20], Loss: nan\n",
      "Epoch [3/20], Loss: nan\n",
      "Epoch [4/20], Loss: nan\n",
      "Epoch [5/20], Loss: nan\n",
      "Epoch [6/20], Loss: nan\n",
      "Epoch [7/20], Loss: nan\n",
      "Epoch [8/20], Loss: nan\n",
      "Epoch [9/20], Loss: nan\n",
      "Epoch [10/20], Loss: nan\n",
      "Epoch [11/20], Loss: nan\n",
      "Epoch [12/20], Loss: nan\n",
      "Epoch [13/20], Loss: nan\n",
      "Epoch [14/20], Loss: nan\n",
      "Epoch [15/20], Loss: nan\n",
      "Epoch [16/20], Loss: nan\n",
      "Epoch [17/20], Loss: nan\n",
      "Epoch [18/20], Loss: nan\n",
      "Epoch [19/20], Loss: nan\n",
      "Epoch [20/20], Loss: nan\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Ensure device is defined\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Make sure DataLoader variables exist\n",
    "if \"train_loader\" not in locals():\n",
    "    train_loader = dataloader  # From 3.1, using full dataset as training data\n",
    "\n",
    "# Training parameters\n",
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for initial_conditions, final_conditions in train_loader:\n",
    "        # Move data to device\n",
    "        initial_conditions = initial_conditions.to(device)\n",
    "        final_conditions = final_conditions.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(initial_conditions)\n",
    "        loss = criterion(outputs, final_conditions)\n",
    "\n",
    "        # Backward pass + optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * initial_conditions.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10376a49-abc7-43ff-bb73-db5b498ebab3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
