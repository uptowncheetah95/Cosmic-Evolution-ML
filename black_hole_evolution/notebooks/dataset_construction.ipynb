{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f93fa5c2-e076-4ee0-a165-53f815c620fe",
   "metadata": {},
   "source": [
    "## Black Hole Evolution Dataset (TNG100)\n",
    "This notebook builds a dataset for modeling the evolution of black holes using the IllustrisTNG100 simulation (snapshots 18–33). The dataset will be used to train an LSTM to predict future black hole properties based on their past evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8e4222-2bed-44ea-88e0-db6d10fc79a9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 1. Environment Setup\n",
    "---\n",
    "Import necessary libraries and configure global settings for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f86d1a3-9299-473f-9598-c4dc2e2a5f47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.24.3\n",
      "PyTorch version: 2.0.1+cpu\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "\n",
    "random.seed(42)  # Ensures reproducible random sampling later\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12afb858-bb7a-428c-8043-7ebc33b56266",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2. Data Access & Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32112d15-0f14-4247-beca-542772badaf3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2.1 Load Subhalo Catalog (Snapshot 33)\n",
    "---\n",
    "We begin by selecting black-hole-hosting subhalos at snapshot 33 (z ≈ 0, present day)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50a07782-a3fa-4ffa-8613-53045e98db09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total subhalos with black holes: 29415\n"
     ]
    }
   ],
   "source": [
    "import illustris_python as il\n",
    "\n",
    "basePath = \"/home/tnguser/sims.TNG/TNG100-1/output\" # Adjust based on Environment\n",
    "\n",
    "subhalos = il.groupcat.loadSubhalos(\n",
    "    basePath, \n",
    "    33, \n",
    "    fields=['SubhaloBHMass', 'SubhaloMassType']\n",
    ")\n",
    "\n",
    "bh_mass = subhalos['SubhaloBHMass']\n",
    "stellar_mass = subhalos['SubhaloMassType'][:, 4]  # Type 4 = stellar component\n",
    "\n",
    "bh_mask = bh_mass > 0\n",
    "print(f\"Total subhalos with black holes: {bh_mask.sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865d0ab1-776b-447e-937e-c90235a83c95",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### 2.2 Extract Black Hole Evolutionary Histories\n",
    "---\n",
    "Trace each black hole’s most bound progenitor branch through merger trees and store time-ordered properties across snapshots 18-33.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78f32a09-d3bb-40a1-8137-6ffb6719b173",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning, empty return. Subhalo [18022] at snapNum [33] not in tree.\n",
      "Warning, empty return. Subhalo [19588] at snapNum [33] not in tree.\n",
      "Warning, empty return. Subhalo [39449] at snapNum [33] not in tree.\n",
      "Warning, empty return. Subhalo [39547] at snapNum [33] not in tree.\n",
      "Warning, empty return. Subhalo [138375] at snapNum [33] not in tree.\n",
      "Warning, empty return. Subhalo [147013] at snapNum [33] not in tree.\n",
      "Checked 5000/29415 subhalos...\n",
      "Checked 10000/29415 subhalos...\n",
      "Checked 15000/29415 subhalos...\n",
      "Checked 20000/29415 subhalos...\n",
      "Checked 25000/29415 subhalos...\n",
      "Black holes with ≥90% snapshot coverage: 29232\n"
     ]
    }
   ],
   "source": [
    "tree_base = f\"{basePath}/postprocessing/trees/sublink\"\n",
    "\n",
    "full_histories = {}\n",
    "bh_list = [i for i, has_bh in enumerate(bh_mask) if has_bh]\n",
    "required_snaps = set(range(18, 34))\n",
    "\n",
    "for count, sub_id in enumerate(bh_list, start=1):\n",
    "    try:\n",
    "        tree = il.sublink.loadTree(\n",
    "            basePath,\n",
    "            33,\n",
    "            sub_id,\n",
    "            fields=[\n",
    "                'SubhaloID',\n",
    "                'SnapNum',\n",
    "                'SubhaloBHMass',\n",
    "                'SubhaloBHMdot',\n",
    "                'SubhaloMassType',\n",
    "                'SubhaloSFR',\n",
    "                'SubhaloVelDisp'\n",
    "            ],\n",
    "            onlyMPB=True\n",
    "        )\n",
    "\n",
    "        mask = (tree['SnapNum'] <= 32) & (tree['SnapNum'] >= 18)\n",
    "        snaps = set(tree['SnapNum'][mask])\n",
    "\n",
    "        # Keep only if ≥90% snapshot coverage\n",
    "        if len(snaps & required_snaps) >= int(0.9 * len(required_snaps)):\n",
    "            sorted_idx = np.argsort(tree['SnapNum'][mask])\n",
    "            full_histories[sub_id] = {\n",
    "                \"snap_nums\": tree['SnapNum'][mask][sorted_idx].tolist(),\n",
    "                \"bh_mass\": tree['SubhaloBHMass'][mask][sorted_idx].tolist(),\n",
    "                \"bh_accretion\": tree['SubhaloBHMdot'][mask][sorted_idx].tolist(),\n",
    "                \"stellar_mass\": tree['SubhaloMassType'][mask][sorted_idx, 4].tolist(),\n",
    "                \"halo_mass\": tree['SubhaloMassType'][mask][sorted_idx].sum(axis=1).tolist(),\n",
    "                \"sfr\": tree['SubhaloSFR'][mask][sorted_idx].tolist(),\n",
    "                \"vel_dispersion\": tree['SubhaloVelDisp'][mask][sorted_idx].tolist()\n",
    "            }\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    if count % 5000 == 0:\n",
    "        print(f\"Checked {count}/{len(bh_list)} subhalos...\", flush=True)\n",
    "\n",
    "print(f\"Black holes with ≥90% snapshot coverage: {len(full_histories)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00c3f48-ff21-4ec6-b8e8-7bddf24605fd",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.3 Sample and Verify Black Holes\n",
    "---\n",
    "Randomly select 2,500 black holes with ≥90% snapshot coverage \n",
    "and inspect one example to confirm that time-series properties \n",
    "were extracted correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84c22993-39b3-40b4-83ec-2dc7f0461641",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 2500 black holes.\n"
     ]
    }
   ],
   "source": [
    "sampled_ids = random.sample(list(full_histories.keys()), 2500)\n",
    "print(f\"Sampled {len(sampled_ids)} black holes.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3795aed-5727-4d63-b73b-a614d85cd68e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inspecting black hole ID: 334728\n",
      "Snapshots: [18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32]\n",
      "BH Mass Sequence: [0.0, 0.0, 0.0, 0.0, 8.097196405287832e-05, 8.336849714396521e-05, 8.618094580015168e-05, 9.101477917283773e-05, 9.366880112793297e-05, 9.904281614581123e-05, 0.000103422709798906, 0.00010685356392059475, 0.0001107670905184932, 0.00011433172039687634, 0.00011698293383233249]\n"
     ]
    }
   ],
   "source": [
    "# Verification\n",
    "check_id = random.choice(sampled_ids)\n",
    "print(f\"Inspecting black hole ID: {check_id}\")\n",
    "print(\"Snapshots:\", full_histories[check_id][\"snap_nums\"])\n",
    "print(\"BH Mass Sequence:\", full_histories[check_id][\"bh_mass\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018ab118-ab5c-4cd5-9f15-3b286146024c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2.4 Prepare Dataset for Machine Learning\n",
    "---\n",
    "Structure the extracted properties into a consistent time-series format and save as a CSV for future training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65f6625b-5b36-487c-b095-272336a193ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Parse *_snapXX columns\u001b[39;00m\n\u001b[1;32m     10\u001b[0m snap_col_pattern \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39mcompile(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m^(?P<feature>.+)_snap(?P<snapshot>\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124md+)$\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m parsed \u001b[38;5;241m=\u001b[39m [(c, m\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m), \u001b[38;5;28mint\u001b[39m(m\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnapshot\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[0;32m---> 12\u001b[0m           \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[1;32m     13\u001b[0m           \u001b[38;5;28;01mif\u001b[39;00m (m \u001b[38;5;241m:=\u001b[39m snap_col_pattern\u001b[38;5;241m.\u001b[39mmatch(c))]\n\u001b[1;32m     15\u001b[0m parse_map \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(parsed, columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msnapshot\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Melt wide → long\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "ID_COL = \"subhalo_id\"\n",
    "FEATURE_ORDER = [\"bh_mass\", \"bh_acc\", \"stellar_mass\", \"sfr\", \"halo_mass\", \"vel_disp\"]\n",
    "OUT_CSV = \"../data/black_hole_evolution_tng100.csv\"\n",
    "\n",
    "# Parse *_snapXX columns\n",
    "snap_col_pattern = re.compile(r\"^(?P<feature>.+)_snap(?P<snapshot>\\d+)$\")\n",
    "parsed = [(c, m.group(\"feature\"), int(m.group(\"snapshot\")))\n",
    "          for c in df.columns\n",
    "          if (m := snap_col_pattern.match(c))]\n",
    "\n",
    "parse_map = pd.DataFrame(parsed, columns=[\"col\", \"feature\", \"snapshot\"])\n",
    "\n",
    "# Melt wide → long\n",
    "long = df[[ID_COL] + parse_map[\"col\"].tolist()].melt(\n",
    "    id_vars=[ID_COL], var_name=\"col\", value_name=\"value\"\n",
    ").merge(parse_map, on=\"col\").drop(columns=[\"col\"])\n",
    "\n",
    "# Pivot to one row per (ID, snapshot)\n",
    "tidy = long.pivot(index=[ID_COL, \"snapshot\"], columns=\"feature\", values=\"value\").reset_index()\n",
    "tidy[\"snapshot\"] = tidy[\"snapshot\"].astype(int)\n",
    "tidy = tidy.sort_values([ID_COL, \"snapshot\"]).reset_index(drop=True)\n",
    "\n",
    "# Add missing expected features\n",
    "for f in FEATURE_ORDER:\n",
    "    if f not in tidy.columns:\n",
    "        tidy[f] = np.nan\n",
    "\n",
    "tidy = tidy[[ID_COL, \"snapshot\"] + FEATURE_ORDER + [c for c in tidy.columns if c not in [ID_COL, \"snapshot\"] + FEATURE_ORDER]]\n",
    "\n",
    "tidy.to_csv(OUT_CSV, index=False)\n",
    "print(f\"[OK] Saved long-format dataset to: {OUT_CSV}\")\n",
    "tidy.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35700c21-2e4e-4577-be59-907f9fa3e9dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
